# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: {{ composant.cluster_name }}
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: {{ inventory_hostname }}
# TODO: Better handling of this as we have to modify wich nodes are requested by logstash / kibana
node.master: {{ is_master|default('true') }}
node.data: {{ is_data|default('true') }}
node.ingest: {{ is_ingest|default('false') }}
node.ml: false
xpack.ml.enabled: false
#
# Add custom attributes to the node:
#
# node.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: {{ elasticsearch_data_dir }}
#
# Path to log files:
#
path.logs: {{ elasticsearch_log_dir }}

#
# Path for backup/snapshots:
#
{% if (composant.repo is defined) and (composant.repo|length > 0) and ("" not in composant.repo) %}
path.repo: ["{{ composant.repo | list | join ('\',\'') }}"]
{% endif %}

#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
# = Disable swapping
bootstrap.memory_lock: true
#
# Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory
# available on the system and that the owner of the process is allowed to use this limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
# Note : if installing to localhost, notably a docker container, we need to bind larger than localhost
{% if inventory_hostname in single_vm_hostnames %}
network.host: {{ composant.network_host | default('0.0.0.0') }}
http.cors.enabled: true
http.cors.allow-origin: "*"
{% else %}
# KWA TODO: Check it again (ansible_hostname VS inventory_hostname VS ip_service)
network.host: {{ ip_admin }}
{% endif %}
# Set a custom port for HTTP:
#
http.port: {{ composant.port_http }}
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when this node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
discovery.seed_hosts: [ {% for host in groups['hosts_elasticsearch_log'] %}"{{ hostvars[host]['ip_admin'] }}"{% if not loop.last %},{% endif %}{% endfor %} ]
#
# Bootstrap the cluster using an initial set of master-eligible nodes:
#
# TODO OMA : faire mieux, plus propre et prenant bien en compte is_master de chaque membre
cluster.initial_master_nodes: [ {% for host in groups['hosts_elasticsearch_log'] %}"{{ hostvars[host]['ip_admin'] }}"{% if not loop.last %},{% endif %}{% endfor %} ]
#
# For more information, consult the discovery and cluster formation module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
action.destructive_requires_name: true

# related to https://www.elastic.co/guide/en/elasticsearch/reference/7.3/modules-fielddata.html
indices.fielddata.cache.size: {{ composant.indices_fielddata_cache_size }}

# related to https://www.elastic.co/guide/en/elasticsearch/reference/7.3/circuit-breaker.html#fielddata-circuit-breaker
indices.breaker.fielddata.limit: {{ composant.indices_breaker_fielddata_limit }}

indices.mapping.dynamic_timeout: {{ composant.dynamic_timeout|default('30s') }}

# thread_pool configuration
thread_pool:
    analyze:
        size: {{ (ansible_processor_cores * ansible_processor_threads_per_core) | round (0, 'floor') | int }}
        queue_size: 5000
    get:
        size: {{ elasticsearch.log.thread_pool.get.size |default((ansible_processor_cores * ansible_processor_threads_per_core)| round (0, 'floor') | int) }}
        queue_size: 1000
    search:
        size: {{ elasticsearch.log.thread_pool.search.size |default(((ansible_processor_cores * ansible_processor_threads_per_core * 3 / 2) + 1) | round (0, 'floor') | int) }}
        queue_size: 1000
    write:
        size: {{ elasticsearch.log.thread_pool.write.size |default((ansible_processor_cores * ansible_processor_threads_per_core + 1)| round (0, 'floor') | int) }}
        queue_size: 5000
    warmer:
        core: 1
        max: {{ elasticsearch.log.thread_pool.warmer.max |default(((ansible_processor_cores * ansible_processor_threads_per_core / 2) + 0.5) | round (0, 'floor') | int) }}
        keep_alive: 2m

{% if groups['hosts_elasticsearch_log']|length > 1 %}
# related to affinity and balancing between racks / rooms https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html
cluster.routing.allocation.awareness.attributes: rack_id
node.attr.rack_id: {{ is_balancing|default(vitam_site_name) }}
{% endif %}

# Related to https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-settings.html
xpack.ilm.enabled: false
indices.lifecycle.history_index_enabled: true

indices.breaker.total.use_real_memory: false

# More tuning
xpack.security.enabled: false
xpack.watcher.enabled: false
